{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "865b61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_visualization.ipynb\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055c856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(epoch, folder=\"node_embeddings\"):\n",
    "    \"\"\"\n",
    "    Loads saved embeddings, SK_ID_CURR values, and cluster labels for a given epoch.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): Epoch number used during saving\n",
    "        folder (str): Folder where intermediate .pt and .npy files are stored\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: node embeddings of shape (N, D)\n",
    "        np.ndarray: corresponding SK_ID_CURR values of shape (N,)\n",
    "        np.ndarray: corresponding cluster labels of shape (N,)\n",
    "    \"\"\"\n",
    "    embedding_path = os.path.join(folder, f\"val_embeddings_epoch_{epoch}.pt\")\n",
    "    sk_id_path = os.path.join(folder, f\"val_sk_ids_epoch_{epoch}.npy\")\n",
    "    cluster_path = os.path.join(folder, f\"val_clusters_epoch_{epoch}.npy\")\n",
    "    \n",
    "    embeddings = torch.load(embedding_path, map_location='cpu')\n",
    "    sk_ids = np.load(sk_id_path)\n",
    "    clusters = np.load(cluster_path)\n",
    "\n",
    "    return embeddings, sk_ids, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6930e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_embeddings_tsne(embeddings, sk_ids=None, clusters=None, annotate=True, title=\"t-SNE of Node Embeddings\", perplexity=30, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualizes node embeddings using t-SNE.\n",
    "\n",
    "    Args:\n",
    "        embeddings (torch.Tensor): Node embeddings [N, D]\n",
    "        sk_ids (np.ndarray): SK_ID_CURR values for annotation (optional)\n",
    "        clusters (np.ndarray): Cluster labels for color-coding (optional)\n",
    "        annotate (bool): Whether to annotate selected points with SK_IDs\n",
    "        title (str): Plot title\n",
    "        perplexity (int): t-SNE perplexity\n",
    "        random_state (int): t-SNE random seed\n",
    "    \"\"\"\n",
    "    embeddings_np = embeddings.detach().cpu().numpy()\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state, init='pca')\n",
    "    reduced = tsne.fit_transform(embeddings_np)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if clusters is not None:\n",
    "        plt.scatter(reduced[:, 0], reduced[:, 1], c=clusters, cmap='tab10', alpha=0.6)\n",
    "    else:\n",
    "        plt.scatter(reduced[:, 0], reduced[:, 1], alpha=0.6)\n",
    "\n",
    "    if annotate and sk_ids is not None:\n",
    "        for i in range(len(sk_ids)):\n",
    "            if i % max(1, len(sk_ids) // 100) == 0:\n",
    "                plt.text(reduced[i, 0], reduced[i, 1], str(sk_ids[i]), fontsize=6, alpha=0.5)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9a355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit_risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
